{
  "app": {
    "title": "Quasi-Newton Method Visualization",
    "subtitle": "Interactive comparison of optimization algorithms",
    "footer": "Built with React, D3.js, and TypeScript. Explore how quasi-Newton methods approximate the Hessian matrix."
  },
  "controls": {
    "objectiveFunction": "Objective Function",
    "algorithms": "Algorithms",
    "startPoint": "Start Point",
    "startPointHint": "Or click on the plot to set start point",
    "runOptimization": "Run Optimization",
    "trueHessian": "True Hessian"
  },
  "iteration": {
    "title": "Iteration: {{current}} / {{max}}",
    "speed": "Speed: {{speed}}x",
    "reset": "Reset",
    "previous": "Previous",
    "play": "Play",
    "pause": "Pause",
    "next": "Next"
  },
  "visualization": {
    "optimizationPath": "Optimization Path",
    "viewMode": "View Mode",
    "view2D": "2D Contour",
    "view3D": "3D Surface",
    "resetCamera": "Reset Camera"
  },
  "comparison": {
    "title": "Algorithm Comparison",
    "empty": "Select algorithms and click on the plot to set a starting point.",
    "algorithm": "Algorithm",
    "iterations": "Iterations",
    "finalValue": "Final Value",
    "converged": "Converged",
    "detailedStats": "Detailed Statistics",
    "solution": "Solution",
    "gradientNorm": "Gradient Norm",
    "functionEvals": "Function Evals",
    "gradientEvals": "Gradient Evals"
  },
  "hessian": {
    "title": "Iteration {{iteration}}: Hessian Comparison",
    "trueHessian": "True Hessian H",
    "trueInverse": "True H⁻¹",
    "approxInverse": "Approx H⁻¹ (Bₖ)",
    "error": "Error (Bₖ - H⁻¹)",
    "frobeniusError": "Frobenius Error",
    "relativeError": "Relative Error",
    "explanation": {
      "trueHessian": "The exact second-order derivatives of the objective function.",
      "approxInverse": "The quasi-Newton approximation to the inverse Hessian, built incrementally using gradient information.",
      "convergence": "As iterations progress, quasi-Newton methods (BFGS, DFP, SR1) improve their approximation of the inverse Hessian, leading to faster convergence near the optimum."
    }
  },
  "explanation": {
    "title": "Understanding Quasi-Newton Methods",
    "steepestDescent": {
      "title": "Gradient Descent (Steepest Descent)",
      "description": "The simplest optimization method. Uses only the gradient to determine the search direction: d = -\u2207f. Simple but can be slow for ill-conditioned problems due to zigzag behavior.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Gradient_descent" }
      ]
    },
    "bb": {
      "title": "Barzilai-Borwein (BB)",
      "description": "The simplest quasi-Newton method. Approximates the inverse Hessian as a scalar times identity: Bₖ = αₖI. The scalar αₖ is derived from the secant condition, making it a 'spectral gradient method'.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Barzilai-Borwein_method" }
      ]
    },
    "newton": {
      "title": "Newton's Method",
      "description": "Uses the exact Hessian matrix H(x) to compute the search direction: d = -H⁻¹∇f. Provides quadratic convergence but requires computing and inverting the Hessian at each step.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" },
        { "label": "MathWorld", "url": "https://mathworld.wolfram.com/NewtonsMethod.html" }
      ]
    },
    "trustRegion": {
      "title": "Trust Region (SQP)",
      "description": "Solves a quadratic programming (QP) subproblem at each step: minimize the quadratic model subject to ||d|| ≤ Δ. This is the core idea of Sequential Quadratic Programming (SQP). The trust region Δ is adjusted based on model accuracy.",
      "links": [
        { "label": "Trust Region (Wikipedia)", "url": "https://en.wikipedia.org/wiki/Trust_region" },
        { "label": "SQP (Wikipedia)", "url": "https://en.wikipedia.org/wiki/Sequential_quadratic_programming" }
      ]
    },
    "bfgs": {
      "title": "BFGS",
      "description": "Builds an approximation Bₖ to the inverse Hessian using only gradient information. Updates the approximation to satisfy the secant condition: Bₖ₊₁yₖ = sₖ.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" }
      ]
    },
    "dfp": {
      "title": "DFP",
      "description": "The first quasi-Newton method, proposed by Davidon and later developed by Fletcher and Powell. Uses a different update formula than BFGS but achieves similar results.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula" }
      ]
    },
    "sr1": {
      "title": "SR1 (Symmetric Rank-1)",
      "description": "Uses a rank-1 update that can capture negative curvature. Can exactly reconstruct the Hessian for quadratic functions in n steps, but doesn't guarantee positive definiteness.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Symmetric_rank-one" }
      ]
    },
    "references": {
      "title": "General References",
      "quasiNewton": { "label": "Quasi-Newton Methods (Wikipedia)", "url": "https://en.wikipedia.org/wiki/Quasi-Newton_method" },
      "nocedal": { "label": "Numerical Optimization (Nocedal & Wright)", "url": "https://link.springer.com/book/10.1007/978-0-387-40065-5" }
    }
  },
  "functions": {
    "rosenbrock": {
      "name": "Rosenbrock",
      "description": "f(x,y) = (1 - x)² + 100(y - x²)²"
    },
    "himmelblau": {
      "name": "Himmelblau",
      "description": "f(x,y) = (x² + y - 11)² + (x + y² - 7)²"
    },
    "quadratic": {
      "name": "Quadratic",
      "description": "f(x,y) = ½(4x² + 2xy + 2y²)"
    },
    "illConditionedQuadratic": {
      "name": "Ill-conditioned Quadratic",
      "description": "f(x,y) = ½(100x² + y²)"
    },
    "beale": {
      "name": "Beale",
      "description": "f(x,y) = (1.5 - x + xy)² + (2.25 - x + xy²)² + (2.625 - x + xy³)²"
    },
    "booth": {
      "name": "Booth",
      "description": "f(x,y) = (x + 2y - 7)² + (2x + y - 5)²"
    },
    "matyas": {
      "name": "Matyas",
      "description": "f(x,y) = 0.26(x² + y²) - 0.48xy"
    },
    "threeHumpCamel": {
      "name": "Three-Hump Camel",
      "description": "f(x,y) = 2x² - 1.05x⁴ + x⁶/6 + xy + y²"
    },
    "sphere": {
      "name": "Sphere",
      "description": "f(x,y) = x² + y²"
    },
    "levi": {
      "name": "Lévi N.13",
      "description": "f(x,y) = sin²(3πx) + (x-1)²(1+sin²(3πy)) + (y-1)²(1+sin²(2πy))"
    },
    "rastrigin": {
      "name": "Rastrigin",
      "description": "f(x,y) = 20 + x² - 10cos(2πx) + y² - 10cos(2πy)"
    },
    "ackley": {
      "name": "Ackley",
      "description": "f(x,y) = -20exp(-0.2√(0.5(x²+y²))) - exp(0.5(cos2πx+cos2πy)) + e + 20"
    },
    "goldsteinPrice": {
      "name": "Goldstein-Price",
      "description": "Complex polynomial with minimum at (0, -1)"
    },
    "easom": {
      "name": "Easom",
      "description": "f(x,y) = -cos(x)cos(y)exp(-((x-π)²+(y-π)²))"
    },
    "styblinskiTang": {
      "name": "Styblinski-Tang",
      "description": "f(x,y) = 0.5[(x⁴-16x²+5x) + (y⁴-16y²+5y)]"
    }
  },
  "optimizers": {
    "steepestDescent": {
      "name": "Gradient Descent",
      "description": "Steepest descent using negative gradient direction"
    },
    "bb": {
      "name": "Barzilai-Borwein",
      "description": "Simplest quasi-Newton with scalar Hessian approximation"
    },
    "newton": {
      "name": "Newton's Method",
      "description": "Uses true Hessian matrix for quadratic convergence"
    },
    "trustRegion": {
      "name": "Trust Region",
      "description": "Solves QP subproblem with trust region constraint"
    },
    "bfgs": {
      "name": "BFGS",
      "description": "Broyden-Fletcher-Goldfarb-Shanno quasi-Newton method"
    },
    "dfp": {
      "name": "DFP",
      "description": "Davidon-Fletcher-Powell quasi-Newton method"
    },
    "sr1": {
      "name": "SR1",
      "description": "Symmetric Rank-1 quasi-Newton method"
    }
  },
  "language": {
    "label": "Language",
    "en": "English",
    "ja": "Japanese"
  },
  "stepDetails": {
    "updateFormula": "Update Formula",
    "updateRule": "update rule",
    "lineSearchDesc": "α is determined by line search (Armijo/Wolfe conditions) to ensure sufficient decrease",
    "alphaDesc": "Step size found by line search satisfying descent conditions",
    "improvement": "Improvement",
    "hessianApproxDesc": "Quasi-Newton approximation to inverse Hessian, updated each iteration using gradient changes",
    "seeMatrixComparison": "See matrix comparison panel for details",
    "bbExplanation": "This is the BB1 formula. It comes from minimizing ‖αy - s‖² which means finding α such that αI best approximates the inverse Hessian in the secant sense. This is the simplest quasi-Newton method!",
    "qpSubproblem": "QP Subproblem",
    "qpExplanation": "This quadratic programming (QP) subproblem is the heart of SQP. The quadratic model m_k(d) approximates f(x_k + d) using Taylor expansion. The constraint ||d|| ≤ Δ_k limits the step to a \"trust region\" where the model is reliable.",
    "trustRegionExplanation": "The ratio ρ_k measures how well the quadratic model predicted the actual reduction. If ρ_k is large, we trust the model and expand Δ. If ρ_k is small, the model was inaccurate so we shrink Δ."
  }
}
