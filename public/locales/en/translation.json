{
  "app": {
    "title": "Quasi-Newton Visualization",
    "subtitle": "Interactive comparison of optimization algorithms",
    "footer": "© Luma · Made with Claude Code"
  },
  "controls": {
    "objectiveFunction": "Objective Function",
    "algorithms": "Algorithms",
    "startPoint": "Start Point",
    "startPointHint": "Or click on the plot to set start point",
    "trueHessian": "True Hessian"
  },
  "iteration": {
    "title": "Iteration: {{current}} / {{max}}",
    "speed": "Speed: {{speed}}x",
    "reset": "Reset",
    "previous": "Previous",
    "play": "Play",
    "pause": "Pause",
    "next": "Next"
  },
  "visualization": {
    "optimizationPath": "Optimization Path",
    "viewMode": "View Mode",
    "view2D": "2D Contour",
    "view3D": "3D Surface",
    "resetCamera": "Reset Camera"
  },
  "comparison": {
    "title": "Algorithm Comparison",
    "empty": "Select algorithms and click on the plot to set a starting point.",
    "algorithm": "Algorithm",
    "iterations": "Iterations",
    "finalValue": "Final Value",
    "converged": "Converged",
    "detailedStats": "Detailed Statistics",
    "solution": "Solution",
    "gradientNorm": "Gradient Norm",
    "functionEvals": "Function Evals",
    "gradientEvals": "Gradient Evals"
  },
  "hessian": {
    "title": "Iteration {{iteration}}: Hessian Comparison",
    "trueHessian": "True Hessian H",
    "trueInverse": "True H⁻¹",
    "approxInverse": "Approx H⁻¹ (Bₖ)",
    "error": "Error (Bₖ - H⁻¹)",
    "frobeniusError": "Frobenius Error",
    "relativeError": "Relative Error",
    "explanation": {
      "trueHessian": "The exact second-order derivatives of the objective function.",
      "approxInverse": "The quasi-Newton approximation to the inverse Hessian, built incrementally using gradient information.",
      "convergence": "As iterations progress, quasi-Newton methods (BFGS, DFP, SR1) improve their approximation of the inverse Hessian, leading to faster convergence near the optimum."
    }
  },
  "explanation": {
    "title": "Understanding Quasi-Newton Methods",
    "steepestDescent": {
      "title": "Gradient Descent (Steepest Descent)",
      "description": "The simplest optimization method. Uses only the gradient to determine the search direction: d = -\u2207f. Simple but can be slow for ill-conditioned problems due to zigzag behavior.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Gradient_descent" }
      ]
    },
    "bb": {
      "title": "Barzilai-Borwein (BB)",
      "description": "The simplest quasi-Newton method. Approximates the inverse Hessian as a scalar times identity: Bₖ = αₖI. The scalar αₖ is derived from the secant condition, making it a 'spectral gradient method'.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Barzilai-Borwein_method" }
      ]
    },
    "newton": {
      "title": "Newton's Method",
      "description": "Uses the exact Hessian matrix H(x) to compute the search direction: d = -H⁻¹∇f. Provides quadratic convergence but requires computing and inverting the Hessian at each step.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" },
        { "label": "MathWorld", "url": "https://mathworld.wolfram.com/NewtonsMethod.html" }
      ]
    },
    "trustRegion": {
      "title": "Trust Region (SQP)",
      "description": "Solves a quadratic programming (QP) subproblem at each step: minimize the quadratic model subject to ||d|| ≤ Δ. This is the core idea of Sequential Quadratic Programming (SQP). The trust region Δ is adjusted based on model accuracy.",
      "links": [
        { "label": "Trust Region (Wikipedia)", "url": "https://en.wikipedia.org/wiki/Trust_region" },
        { "label": "SQP (Wikipedia)", "url": "https://en.wikipedia.org/wiki/Sequential_quadratic_programming" }
      ]
    },
    "bfgs": {
      "title": "BFGS",
      "description": "Builds an approximation Bₖ to the inverse Hessian using only gradient information. Updates the approximation to satisfy the secant condition: Bₖ₊₁yₖ = sₖ.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" }
      ]
    },
    "dfp": {
      "title": "DFP",
      "description": "The first quasi-Newton method, proposed by Davidon and later developed by Fletcher and Powell. Uses a different update formula than BFGS but achieves similar results.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula" }
      ]
    },
    "sr1": {
      "title": "SR1 (Symmetric Rank-1)",
      "description": "Uses a rank-1 update that can capture negative curvature. Can exactly reconstruct the Hessian for quadratic functions in n steps, but doesn't guarantee positive definiteness.",
      "links": [
        { "label": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Symmetric_rank-one" }
      ]
    },
    "references": {
      "title": "General References",
      "quasiNewton": { "label": "Quasi-Newton Methods (Wikipedia)", "url": "https://en.wikipedia.org/wiki/Quasi-Newton_method" },
      "nocedal": { "label": "Numerical Optimization (Nocedal & Wright)", "url": "https://link.springer.com/book/10.1007/978-0-387-40065-5" }
    }
  },
  "functions": {
    "rosenbrock": {
      "name": "Rosenbrock",
      "description": "f(x,y) = (1 - x)² + 100(y - x²)²"
    },
    "himmelblau": {
      "name": "Himmelblau",
      "description": "f(x,y) = (x² + y - 11)² + (x + y² - 7)²"
    },
    "quadratic": {
      "name": "Quadratic",
      "description": "f(x,y) = ½(4x² + 2xy + 2y²)"
    },
    "illConditionedQuadratic": {
      "name": "Ill-conditioned Quadratic",
      "description": "f(x,y) = ½(100x² + y²)"
    },
    "beale": {
      "name": "Beale",
      "description": "f(x,y) = (1.5 - x + xy)² + (2.25 - x + xy²)² + (2.625 - x + xy³)²"
    },
    "booth": {
      "name": "Booth",
      "description": "f(x,y) = (x + 2y - 7)² + (2x + y - 5)²"
    },
    "matyas": {
      "name": "Matyas",
      "description": "f(x,y) = 0.26(x² + y²) - 0.48xy"
    },
    "threeHumpCamel": {
      "name": "Three-Hump Camel",
      "description": "f(x,y) = 2x² - 1.05x⁴ + x⁶/6 + xy + y²"
    },
    "sphere": {
      "name": "Sphere",
      "description": "f(x,y) = x² + y²"
    },
    "trid": {
      "name": "Trid",
      "description": "f(x,y) = (x-1)² + (y-1)² - xy"
    },
    "zakharov": {
      "name": "Zakharov",
      "description": "f(x,y) = x² + y² + (0.5x + y)² + (0.5x + y)⁴"
    },
    "sumSquares": {
      "name": "Sum of Squares",
      "description": "f(x,y) = x² + 2y²"
    },
    "dixonPrice": {
      "name": "Dixon-Price",
      "description": "f(x,y) = (x - 1)² + 2(2y² - x)²"
    },
    "rotatedEllipsoid": {
      "name": "Rotated Ellipsoid",
      "description": "f(x,y) = x² + (x + y)²"
    },
    "levi": {
      "name": "Lévi N.13",
      "description": "f(x,y) = sin²(3πx) + (x-1)²(1+sin²(3πy)) + (y-1)²(1+sin²(2πy))"
    },
    "rastrigin": {
      "name": "Rastrigin",
      "description": "f(x,y) = 20 + x² - 10cos(2πx) + y² - 10cos(2πy)"
    },
    "ackley": {
      "name": "Ackley",
      "description": "f(x,y) = -20exp(-0.2√(0.5(x²+y²))) - exp(0.5(cos2πx+cos2πy)) + e + 20"
    },
    "goldsteinPrice": {
      "name": "Goldstein-Price",
      "description": "Complex polynomial with minimum at (0, -1)"
    },
    "easom": {
      "name": "Easom",
      "description": "f(x,y) = -cos(x)cos(y)exp(-((x-π)²+(y-π)²))"
    },
    "styblinskiTang": {
      "name": "Styblinski-Tang",
      "description": "f(x,y) = 0.5[(x⁴-16x²+5x) + (y⁴-16y²+5y)]"
    },
    "mccormick": {
      "name": "McCormick",
      "description": "f(x,y) = sin(x+y) + (x-y)² - 1.5x + 2.5y + 1"
    },
    "sixHumpCamel": {
      "name": "Six-Hump Camel",
      "description": "f(x,y) = (4-2.1x²+x⁴/3)x² + xy + (-4+4y²)y²"
    },
    "schaffer2": {
      "name": "Schaffer N.2",
      "description": "f(x,y) = 0.5 + (sin²(x²-y²)-0.5)/(1+0.001(x²+y²))²"
    },
    "bukin6": {
      "name": "Bukin N.6",
      "description": "f(x,y) = 100√|y-0.01x²| + 0.01|x+10|"
    },
    "crossInTray": {
      "name": "Cross-in-Tray",
      "description": "f(x,y) = -0.0001(|sin(x)sin(y)exp(...)| + 1)^0.1"
    },
    "holderTable": {
      "name": "Holder Table",
      "description": "f(x,y) = -|sin(x)cos(y)exp(|1-√(x²+y²)/π|)|"
    },
    "dropWave": {
      "name": "Drop-Wave",
      "description": "f(x,y) = -(1+cos(12√(x²+y²)))/(0.5(x²+y²)+2)"
    },
    "branin": {
      "name": "Branin",
      "description": "f(x,y) = (y-bx²+cx-r)² + s(1-t)cos(x) + s"
    }
  },
  "optimizers": {
    "steepestDescent": {
      "name": "Gradient Descent",
      "description": "Steepest descent using negative gradient direction"
    },
    "bb": {
      "name": "Barzilai-Borwein",
      "description": "Simplest quasi-Newton with scalar Hessian approximation"
    },
    "newton": {
      "name": "Newton's Method",
      "description": "Uses true Hessian matrix for quadratic convergence"
    },
    "trustRegion": {
      "name": "Trust Region",
      "description": "Solves QP subproblem with trust region constraint"
    },
    "bfgs": {
      "name": "BFGS",
      "description": "Broyden-Fletcher-Goldfarb-Shanno quasi-Newton method"
    },
    "dfp": {
      "name": "DFP",
      "description": "Davidon-Fletcher-Powell quasi-Newton method"
    },
    "sr1": {
      "name": "SR1",
      "description": "Symmetric Rank-1 quasi-Newton method"
    }
  },
  "language": {
    "label": "Language",
    "en": "English",
    "ja": "Japanese"
  },
  "convergence": {
    "title": "Convergence Analysis",
    "functionValue": "Function Value f(x)",
    "gradientNorm": "Gradient Norm ||∇f||",
    "stepSize": "Step Size α",
    "trustRegionRadius": "Trust Region Δ"
  },
  "lineSearch": {
    "title": "Line Search",
    "noData": "Select an iteration with a search direction",
    "armijoCondition": "Armijo condition: f(x + αd) ≤ f(x) + c₁α∇f·d"
  },
  "eigenvalue": {
    "title": "Eigenvalue Analysis (k={{iteration}})",
    "trueHessian": "True Hessian",
    "trueInverse": "True Inverse",
    "approxInverse": "Approx. Inverse",
    "conditionNumber": "Condition κ",
    "explanation": "Large eigenvalue ratios (condition numbers) indicate ill-conditioning, causing slow convergence. Quasi-Newton methods aim to approximate the Hessian's eigenstructure to improve convergence."
  },
  "secant": {
    "title": "Secant Condition",
    "noData": "Need at least 2 iterations",
    "description": "The inverse Hessian approximation B must satisfy this condition",
    "error": "Error ||Bₖy - s||",
    "relativeError": "Relative Error",
    "curvature": "curvature",
    "explanation": "The secant condition ensures the approximation B captures how the gradient changes along the step direction. y^T s > 0 (positive curvature) is required for BFGS to maintain positive definiteness."
  },
  "formulaLegend": {
    "title": "Symbol Reference",
    "xk": { "name": "Position", "desc": "Current point at iteration k. The algorithm seeks to find the minimum by updating this position." },
    "grad": { "name": "Gradient", "desc": "The gradient (first derivatives) of f at xₖ. Points in the direction of steepest increase." },
    "dk": { "name": "Direction", "desc": "Search direction at iteration k. For quasi-Newton: dₖ = -Bₖ∇f. For steepest descent: dₖ = -∇f." },
    "alpha": { "name": "Step size", "desc": "Step length along the search direction, determined by line search to ensure sufficient decrease." },
    "H": { "name": "Hessian", "desc": "Matrix of second partial derivatives. Captures local curvature information of the function." },
    "Bk": { "name": "Inv. Hessian approx.", "desc": "Quasi-Newton approximation to the inverse Hessian, updated each iteration using gradient changes." },
    "sk": { "name": "Step vector", "desc": "The actual step taken: difference between consecutive iterates." },
    "yk": { "name": "Gradient diff.", "desc": "Change in gradient between consecutive iterates. Used to update Bₖ." },
    "Delta": { "name": "Trust region", "desc": "Trust region radius. Limits step size to region where quadratic model is reliable." },
    "rho": { "name": "Reduction ratio", "desc": "Ratio of actual reduction to predicted reduction. Used to adjust trust region." }
  },
  "stepDetails": {
    "updateFormula": "Update Formula",
    "updateRule": "update rule",
    "lineSearchDesc": "α is determined by line search (Armijo/Wolfe conditions) to ensure sufficient decrease",
    "alphaDesc": "Step size found by line search satisfying descent conditions",
    "improvement": "Improvement",
    "hessianApproxDesc": "Quasi-Newton approximation to inverse Hessian, updated each iteration using gradient changes",
    "seeMatrixComparison": "Compare in side panel",
    "bbHessianApprox": "Scalar × Identity",
    "quasiNewtonHessianApprox": "Inverse Hessian Approx",
    "bbExplanation": "This is the BB1 formula. It comes from minimizing ‖αy - s‖² which means finding α such that αI best approximates the inverse Hessian in the secant sense. This is the simplest quasi-Newton method!",
    "qpSubproblem": "QP Subproblem",
    "qpExplanation": "This quadratic programming (QP) subproblem is the heart of SQP. The quadratic model m_k(d) approximates f(x_k + d) using Taylor expansion. The constraint ||d|| ≤ Δ_k limits the step to a \"trust region\" where the model is reliable.",
    "trustRegionExplanation": "The ratio ρ_k measures how well the quadratic model predicted the actual reduction. If ρ_k is large, we trust the model and expand Δ. If ρ_k is small, the model was inaccurate so we shrink Δ.",
    "quadraticModel": "Quadratic Model",
    "quadraticModelExplanation": "This quadratic model uses the quasi-Newton approximation B_k⁻¹ ≈ H. The purple surface shows this approximation, while the orange surface (if available) shows the true Hessian model. Click to pin the visualization.",
    "hoverHint": "Hover to visualize on plot"
  }
}
